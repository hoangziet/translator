{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2a04c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d811650",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747895803.748708  117846 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747895803.753343  117846 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747895803.763642  117846 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747895803.763674  117846 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747895803.763676  117846 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747895803.763677  117846 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8ee62c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 1000\n",
    "BATCH_SIZE = 32 \n",
    "VOCAB_SIZE = 2300\n",
    "UNITS = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91aa5d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There're 9428 tranlsaltions in the datasets\n",
      "The first 5 pairs\n",
      "[['Run!' 'Chạy!']\n",
      " ['Run!' 'Chạy đi!']\n",
      " ['Help!' 'Giúp tôi với!']\n",
      " ['Help!' 'Cứu tôi!']\n",
      " ['Jump!' 'Nhảy đi!']]\n"
     ]
    }
   ],
   "source": [
    "path_to_file = pathlib.Path(\"data/vie-eng/vie.txt\")\n",
    "text = path_to_file.read_text(encoding = \"utf-8\")\n",
    "\n",
    "lines = text.splitlines()\n",
    "pairs = np.array([line.split('\\t')[:2] for line in lines])\n",
    "context, target = pairs[:,0], pairs[:, 1]\n",
    "\n",
    "print(f\"There're {len(lines)} tranlsaltions in the datasets\")\n",
    "print(\"The first 5 pairs\")\n",
    "print(pairs[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f15bacf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1747895807.779282  117846 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2146 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2050, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "context_train, context_val, target_train, target_val = train_test_split(context, target, test_size = 0.2, random_state = 42)\n",
    "train_raw = (\n",
    "    tf.data.Dataset.from_tensor_slices((context_train, target_train))\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE)\n",
    ")\n",
    "val_raw = (\n",
    "    tf.data.Dataset.from_tensor_slices((context_val, target_val))\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76cf64cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4ae8ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    # text = tf.strings.lower(text)\n",
    "    text = tf.strings.lower(text)\n",
    "    text = tf.strings.regex_replace(text, \"[^a-zA-ZÀ-ỹà-ỹ?.,! ]\", '')  # keep Vietnamese words\n",
    "    text = tf.strings.regex_replace(text, \"[?.,!]\", r' \\0 ')\n",
    "    text = tf.strings.strip(text)\n",
    "    text = tf.strings.join([\"[SOS]\", text, \"[EOS]\"], separator=\" \")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54a32e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sentence\n",
      "Bí mật của cuộc sống là ngã bảy lần và đứng dậy tám lần – trích Nhà Giả Kim | Paulo Coelho \n",
      "\n",
      "Example cleaned of example sentence (Not decoded)\n",
      "tf.Tensor(b'[SOS] b\\xc3\\xad m\\xe1\\xba\\xadt c\\xe1\\xbb\\xa7a cu\\xe1\\xbb\\x99c s\\xe1\\xbb\\x91ng l\\xc3\\xa0 ng\\xc3\\xa3 b\\xe1\\xba\\xa3y l\\xe1\\xba\\xa7n v\\xc3\\xa0 \\xc4\\x91\\xe1\\xbb\\xa9ng d\\xe1\\xba\\xady t\\xc3\\xa1m l\\xe1\\xba\\xa7n  tr\\xc3\\xadch nh\\xc3\\xa0 gi\\xe1\\xba\\xa3 kim  paulo coelho [EOS]', shape=(), dtype=string) \n",
      "\n",
      "Decoded\n",
      "[SOS] bí mật của cuộc sống là ngã bảy lần và đứng dậy tám lần  trích nhà giả kim  paulo coelho [EOS]\n"
     ]
    }
   ],
   "source": [
    "example_vie = \"Bí mật của cuộc sống là ngã bảy lần và đứng dậy tám lần – trích Nhà Giả Kim | Paulo Coelho\"\n",
    "ex_vie_out = clean(example_vie)\n",
    "decoded_ex_vie_out = ex_vie_out.numpy().decode()\n",
    "print(\"Example sentence\")\n",
    "print(example_vie,'\\n')\n",
    "\n",
    "print(\"Example cleaned of example sentence (Not decoded)\")\n",
    "print(ex_vie_out, '\\n')\n",
    "print(\"Decoded\")\n",
    "print(decoded_ex_vie_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16727089",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_vectorizer = tf.keras.layers.TextVectorization(standardize = clean,\n",
    "                                                   max_tokens = VOCAB_SIZE,\n",
    "                                                   ragged = True)\n",
    "eng_vectorizer.adapt(train_raw.map(lambda context, target: context))\n",
    "\n",
    "vie_vectorizer = tf.keras.layers.TextVectorization(standardize = clean,\n",
    "                                                   max_tokens = VOCAB_SIZE,\n",
    "                                                   ragged = True)\n",
    "vie_vectorizer.adapt(train_raw.map(lambda context, target: target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3547dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', '[SOS]', '[EOS]', '.', 'tôi', 'không', 'tom', 'bạn', 'có', 'đã', 'là', '?', 'một', 'ấy', 'anh', 'làm', 'của', 'đó', 'sẽ']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2300"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(vie_vectorizer.get_vocabulary()[:20])\n",
    "vie_vectorizer.vocabulary_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b44ba3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', '[SOS]', '[EOS]', '.', 'i', 'to', 'tom', 'the', 'you', '?', 'a', 'is', 'that', 'do', 'he', 'in', 'of', ',', 'this']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2300"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(eng_vectorizer.get_vocabulary()[:20])\n",
    "eng_vectorizer.vocabulary_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f23a4eeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[2, 1182, 88, 28, 9, 3]]>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_vectorizer([\"Hello who are you\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32f63c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1, shape=(), dtype=int64)\n",
      "tf.Tensor(1461, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "eng_word2id = tf.keras.layers.StringLookup(\n",
    "    vocabulary = eng_vectorizer.get_vocabulary(),\n",
    "    mask_token = '',\n",
    "    oov_token = \"[UNK]\"\n",
    ")\n",
    "\n",
    "eng_id2word = tf.keras.layers.StringLookup(\n",
    "    vocabulary = eng_vectorizer.get_vocabulary(),\n",
    "    mask_token = '',\n",
    "    oov_token = \"[UNK]\",\n",
    "    invert = True\n",
    ")\n",
    "\n",
    "vie_id2word = tf.keras.layers.StringLookup(\n",
    "    vocabulary = vie_vectorizer.get_vocabulary(),\n",
    "    mask_token = '',\n",
    "    oov_token = \"[UNK]\",\n",
    "    invert = True\n",
    ")\n",
    "\n",
    "vie_id2word = tf.keras.layers.StringLookup(\n",
    "    vocabulary = vie_vectorizer.get_vocabulary(),\n",
    "    mask_token = '',\n",
    "    oov_token = \"[UNK]\",\n",
    "    invert = True\n",
    ")\n",
    "\n",
    "print(eng_word2id(\"[UNK]\"))\n",
    "print(eng_word2id(\"jump\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "031ab135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(context, target):\n",
    "    context = eng_vectorizer(context).to_tensor()\n",
    "    target = vie_vectorizer(target)\n",
    "    # Use the current word to predict the next word\n",
    "    target_in = target[:, :-1].to_tensor()\n",
    "    target_out = target[:, 1:].to_tensor()\n",
    "    return (context, target_in), target_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8774b55f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'Give that back to me.', shape=(), dtype=string)\n",
      "tf.Tensor(b'Tr\\xe1\\xba\\xa3 c\\xc3\\xa1i \\xc4\\x91\\xc3\\xb3 l\\xe1\\xba\\xa1i cho t\\xc3\\xb4i.', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for _ in train_raw.take(1):\n",
    "    context, output = _ \n",
    "    print(context[0])\n",
    "    print(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f3de4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_raw.map(process_text, tf.data.AUTOTUNE)\n",
    "val_data = val_raw.map(process_text, tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c0db496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 15)\n",
      "(32, 19)\n",
      "(32, 19)\n",
      "tf.Tensor([  2   9  93   1  13  16 423   4   3   0   0   0   0   0   0], shape=(15,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for _ in train_data.take(1):\n",
    "    a, b = _\n",
    "    a1, a2 = a \n",
    "    print(a1.shape)\n",
    "    print(a2.shape)\n",
    "    print(b.shape)\n",
    "    print(a1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fcfe8a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vect_to_text(vect, id2word):\n",
    "    \"\"\"\n",
    "    Convert from vector of ids into the corresponding text\n",
    "\n",
    "    Args:\n",
    "        vect (tf.Tensor): include id of words in the sequence\n",
    "        id2word (tf.keras.layers.StringLookup): invert string lookup for convert id to word\n",
    "\n",
    "    Returns:\n",
    "        str: the converted texts\n",
    "    \"\"\"\n",
    "    words = id2word(vect)\n",
    "    no_pad = tf.boolean_mask(words, words != b'')\n",
    "    text = tf.strings.reduce_join(no_pad, separator = ' ')\n",
    "    return text.numpy().decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "288a561d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([  2   9 312  39  72  58  20  86   5  39  58   9   4   3   0   0], shape=(16,), dtype=int64)\n",
      "[SOS] you probably know more about me than i know about you . [EOS] \n",
      "\n",
      "tf.Tensor(\n",
      "[  2  69   9  63  27  66  55 211  64  11 211  27  55  69   4   0   0   0\n",
      "   0   0   0   0], shape=(22,), dtype=int64)\n",
      "[SOS] cậu có khi biết nhiều về tớ hơn là tớ biết về cậu . \n",
      "\n",
      "tf.Tensor(\n",
      "[ 69   9  63  27  66  55 211  64  11 211  27  55  69   4   3   0   0   0\n",
      "   0   0   0   0], shape=(22,), dtype=int64)\n",
      "cậu có khi biết nhiều về tớ hơn là tớ biết về cậu . [EOS] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x in train_data.take(1):\n",
    "    (context, target_in), target_out = x\n",
    "    train_context0 = context[0]\n",
    "    target_in0 = target_in[0]\n",
    "    target_out0 = target_out[0]\n",
    "    \n",
    "    print(train_context0)\n",
    "    print(vect_to_text(train_context0, eng_id2word), '\\n')\n",
    "    print(target_in0)\n",
    "    print(vect_to_text(target_in0, vie_id2word), '\\n')\n",
    "    print(target_out0)\n",
    "    print(vect_to_text(target_out0, vie_id2word), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c0863b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, vocab_size):\n",
    "        super().__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(\n",
    "            input_dim = vocab_size, \n",
    "            output_dim = units, \n",
    "            mask_zero = True\n",
    "        )\n",
    "        self.rnn = tf.keras.layers.LSTM(\n",
    "            units = units,\n",
    "            return_sequences = True, \n",
    "        )\n",
    "        \n",
    "    def call(self, context):\n",
    "        \"\"\"\n",
    "        Forward pass of this layer\n",
    "\n",
    "        Args:\n",
    "            context (tf.Tensor) : The sentences to translate (B, L) \n",
    "            \n",
    "        Returns:\n",
    "            tf.Tensor: (B, L, D): Encoded sentence to tranlsate\n",
    "        \"\"\"\n",
    "        x = self.embedding(context)\n",
    "        x = self.rnn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "49f32371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 16)\n",
      "(32, 16, 256)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1747895810.945933  117934 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(UNITS, VOCAB_SIZE)\n",
    "encoder_output = encoder(context)\n",
    "\n",
    "print(context.shape)\n",
    "print(encoder_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cde034a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super().__init__()\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(\n",
    "            key_dim = units,\n",
    "            num_heads = 1\n",
    "        )\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "        self.add = tf.keras.layers.Add()\n",
    "        \n",
    "    def call(self, context, target):\n",
    "        \"\"\"Forward pass of Attention\n",
    "\n",
    "        Args:\n",
    "            context (tf.Tensor): (B, L1, D) encoded setence to translate\n",
    "            target (tf.Tensor): (B, L2, D) right shifted translation \n",
    "            \n",
    "        Return:\n",
    "            tf.Tensor: (B, L2, D) cross attention between context and target\n",
    "        \"\"\"\n",
    "        # query is the translation and the value is the context \n",
    "        # default key = value \n",
    "        attn_output = self.mha(\n",
    "            query = target,\n",
    "            value = context\n",
    "        )\n",
    "        x = self.add([target, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "        return x     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "536a8a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vz/miniconda3/envs/cu118/lib/python3.12/site-packages/keras/src/layers/layer.py:932: UserWarning: Layer 'query' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/vz/miniconda3/envs/cu118/lib/python3.12/site-packages/keras/src/layers/layer.py:932: UserWarning: Layer 'key' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/vz/miniconda3/envs/cu118/lib/python3.12/site-packages/keras/src/layers/layer.py:932: UserWarning: Layer 'value' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 22, 256)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vz/miniconda3/envs/cu118/lib/python3.12/site-packages/keras/src/layers/layer.py:932: UserWarning: Layer 'cross_attention' (of type CrossAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "attn = CrossAttention(UNITS)\n",
    "attn_result = attn(\n",
    "    encoder_output,\n",
    "    tf.keras.layers.Embedding(input_dim = VOCAB_SIZE, output_dim = UNITS, mask_zero = True)(target_out)\n",
    ")\n",
    "\n",
    "print(attn_result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1a114cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, vocab_size):\n",
    "        super().__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(\n",
    "            input_dim = vocab_size,\n",
    "            output_dim = units, \n",
    "            mask_zero = True\n",
    "        )\n",
    "        self.pre_attn_rnn = tf.keras.layers.LSTM(\n",
    "            units = units, \n",
    "            return_sequences = True,\n",
    "            return_state = True \n",
    "        )\n",
    "        self.attention = CrossAttention(units)\n",
    "        self.post_attn_rnn = tf.keras.layers.LSTM(\n",
    "            units = units, \n",
    "            return_sequences = True\n",
    "        )\n",
    "        self.dense = tf.keras.layers.Dense(\n",
    "            units = vocab_size, \n",
    "            activation = \"log_softmax\"\n",
    "        )\n",
    "    def call(self,\n",
    "             context,\n",
    "             target,\n",
    "             state = None,\n",
    "             return_state = False):\n",
    "        \"\"\"Forwardpass of Decodeer\n",
    "\n",
    "        Args:\n",
    "            context (tf.Tensor): (B, L1, D) encoded sentence to translate \n",
    "            target (tf.Tensor): (B, L1) the right-shited translation \n",
    "            state (list([tf.Tensor, tf.Tensor]), optional): Hidden state of the pre-attn RNN. Defaults to None.\n",
    "            return_state (bool, optional): Return state of pre-attn LSTM. Defaults to False.\n",
    "        \"\"\"\n",
    "        x = self.embedding(target)\n",
    "        \n",
    "        x, h, c = self.pre_attn_rnn(\n",
    "            x, \n",
    "            initial_state = state\n",
    "        )\n",
    "        \n",
    "        x = self.attention(context, x)\n",
    "        x = self.post_attn_rnn(x)\n",
    "        logits = self.dense(x)\n",
    "        \n",
    "        if return_state: return logits, [h, c]\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "017b82ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vz/miniconda3/envs/cu118/lib/python3.12/site-packages/keras/src/layers/layer.py:932: UserWarning: Layer 'cross_attention_1' (of type CrossAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/vz/miniconda3/envs/cu118/lib/python3.12/site-packages/keras/src/layers/layer.py:932: UserWarning: Layer 'decoder' (of type Decoder) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([32, 22, 2300])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = Decoder(UNITS, VOCAB_SIZE)\n",
    "logits = decoder(encoder_output, target_out)\n",
    "\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "76cd2a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translator(tf.keras.Model):\n",
    "    def __init__(self, units, vocab_size):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(units, vocab_size)\n",
    "        self.decoder = Decoder(units, vocab_size)\n",
    "    def call(self, inputs):\n",
    "        \"\"\"Forward pass of the model\n",
    "\n",
    "        Args:\n",
    "            inputs (tuple(tf.Tensor, tf.Tensor)): (2, B, L) Tuple of context and target \n",
    "        \n",
    "        Returns:\n",
    "            tf.Tensor: (B, L2, V) log_softmax probabilites of predict a particular token\n",
    "        \"\"\"\n",
    "        context, target = inputs \n",
    "        encoded_context = self.encoder(context)\n",
    "        logits = self.decoder(encoded_context, target)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5acc97ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vz/miniconda3/envs/cu118/lib/python3.12/site-packages/keras/src/layers/layer.py:932: UserWarning: Layer 'cross_attention_2' (of type CrossAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/vz/miniconda3/envs/cu118/lib/python3.12/site-packages/keras/src/layers/layer.py:932: UserWarning: Layer 'decoder_1' (of type Decoder) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([32, 22, 2300])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator = Translator(UNITS, VOCAB_SIZE)\n",
    "logits = translator((context, target_in))\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "56925c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_loss(y_true, y_pred):\n",
    "    \n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "    loss = loss_fn(y_true, y_pred)\n",
    "    \n",
    "    # Check which elements of y_true are padding\n",
    "    mask = tf.cast(y_true != 0, loss.dtype)\n",
    "    \n",
    "    loss *= mask\n",
    "    # Return the total.\n",
    "    return tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
    "\n",
    "\n",
    "def masked_acc(y_true, y_pred):\n",
    "    y_pred = tf.argmax(y_pred, axis=-1)\n",
    "    y_pred = tf.cast(y_pred, y_true.dtype)\n",
    "    match = tf.cast(y_true == y_pred, tf.float32)\n",
    "    mask = tf.cast(y_true != 0, tf.float32)\n",
    "    match*= mask\n",
    "\n",
    "    return tf.reduce_sum(match)/tf.reduce_sum(mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4ebeafda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"translator\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"translator\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ encoder_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Encoder</span>)             │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,114,112</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Decoder</span>)             │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,494,204</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ encoder_1 (\u001b[38;5;33mEncoder\u001b[0m)             │ ?                      │     \u001b[38;5;34m1,114,112\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_1 (\u001b[38;5;33mDecoder\u001b[0m)             │ ?                      │     \u001b[38;5;34m2,494,204\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,608,316</span> (13.76 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,608,316\u001b[0m (13.76 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,608,316</span> (13.76 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,608,316\u001b[0m (13.76 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "translator.compile(\n",
    "    optimizer = \"Adam\",\n",
    "    loss = masked_loss,\n",
    "    metrics = [masked_acc, masked_loss]\n",
    ")\n",
    "\n",
    "translator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2f628b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 25ms/step - loss: 5.6584 - masked_acc: 0.1337 - masked_loss: 5.6584 - val_loss: 4.8399 - val_masked_acc: 0.2149 - val_masked_loss: 4.8402\n",
      "Epoch 2/50\n",
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - loss: 4.6587 - masked_acc: 0.2344 - masked_loss: 4.6587 - val_loss: 4.3844 - val_masked_acc: 0.2695 - val_masked_loss: 4.3845\n",
      "Epoch 3/50\n",
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - loss: 4.1657 - masked_acc: 0.2969 - masked_loss: 4.1657 - val_loss: 4.0606 - val_masked_acc: 0.3123 - val_masked_loss: 4.0606\n",
      "Epoch 4/50\n",
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 24ms/step - loss: 3.8133 - masked_acc: 0.3394 - masked_loss: 3.8133 - val_loss: 3.8033 - val_masked_acc: 0.3510 - val_masked_loss: 3.8039\n",
      "Epoch 5/50\n",
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - loss: 3.4932 - masked_acc: 0.3834 - masked_loss: 3.4932 - val_loss: 3.5951 - val_masked_acc: 0.3858 - val_masked_loss: 3.5953\n",
      "Epoch 6/50\n",
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - loss: 3.1960 - masked_acc: 0.4262 - masked_loss: 3.1960 - val_loss: 3.4192 - val_masked_acc: 0.4111 - val_masked_loss: 3.4196\n",
      "Epoch 7/50\n",
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 24ms/step - loss: 2.9026 - masked_acc: 0.4634 - masked_loss: 2.9026 - val_loss: 3.2609 - val_masked_acc: 0.4389 - val_masked_loss: 3.2605\n",
      "Epoch 8/50\n",
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 24ms/step - loss: 2.6524 - masked_acc: 0.4985 - masked_loss: 2.6524 - val_loss: 3.1384 - val_masked_acc: 0.4588 - val_masked_loss: 3.1383\n",
      "Epoch 9/50\n",
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 24ms/step - loss: 2.4082 - masked_acc: 0.5340 - masked_loss: 2.4081 - val_loss: 3.0047 - val_masked_acc: 0.4817 - val_masked_loss: 3.0045\n",
      "Epoch 10/50\n",
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - loss: 2.1693 - masked_acc: 0.5728 - masked_loss: 2.1693 - val_loss: 2.9116 - val_masked_acc: 0.5011 - val_masked_loss: 2.9116\n",
      "Epoch 11/50\n",
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 24ms/step - loss: 1.9476 - masked_acc: 0.6106 - masked_loss: 1.9476 - val_loss: 2.8383 - val_masked_acc: 0.5143 - val_masked_loss: 2.8379\n",
      "Epoch 12/50\n",
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 24ms/step - loss: 1.7491 - masked_acc: 0.6431 - masked_loss: 1.7491 - val_loss: 2.7640 - val_masked_acc: 0.5260 - val_masked_loss: 2.7643\n",
      "Epoch 13/50\n",
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 24ms/step - loss: 1.5719 - masked_acc: 0.6765 - masked_loss: 1.5719 - val_loss: 2.7410 - val_masked_acc: 0.5355 - val_masked_loss: 2.7408\n",
      "Epoch 14/50\n",
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 24ms/step - loss: 1.4028 - masked_acc: 0.7065 - masked_loss: 1.4028 - val_loss: 2.7088 - val_masked_acc: 0.5396 - val_masked_loss: 2.7087\n",
      "Epoch 15/50\n",
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - loss: 1.2564 - masked_acc: 0.7363 - masked_loss: 1.2564 - val_loss: 2.6881 - val_masked_acc: 0.5468 - val_masked_loss: 2.6882\n",
      "Epoch 16/50\n",
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 24ms/step - loss: 1.1258 - masked_acc: 0.7606 - masked_loss: 1.1258 - val_loss: 2.6822 - val_masked_acc: 0.5543 - val_masked_loss: 2.6822\n",
      "Epoch 17/50\n",
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 24ms/step - loss: 0.9966 - masked_acc: 0.7873 - masked_loss: 0.9966 - val_loss: 2.6888 - val_masked_acc: 0.5575 - val_masked_loss: 2.6885\n",
      "Epoch 18/50\n",
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 24ms/step - loss: 0.8921 - masked_acc: 0.8091 - masked_loss: 0.8921 - val_loss: 2.6838 - val_masked_acc: 0.5595 - val_masked_loss: 2.6838\n",
      "Epoch 19/50\n",
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 24ms/step - loss: 0.7949 - masked_acc: 0.8307 - masked_loss: 0.7949 - val_loss: 2.7054 - val_masked_acc: 0.5662 - val_masked_loss: 2.7054\n"
     ]
    }
   ],
   "source": [
    "history = translator.fit(\n",
    "    train_data,\n",
    "    epochs = 50,    \n",
    "    validation_data = val_data,\n",
    "    callbacks = [tf.keras.callbacks.EarlyStopping(patience = 3),\n",
    "                 tf.keras.callbacks.TensorBoard(log_dir = \"./logs\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2bb2c939",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_next_token(\n",
    "    decoder,\n",
    "    context, \n",
    "    next_token,\n",
    "    state, \n",
    "    done, \n",
    "    temperature = 0.0\n",
    "):\n",
    "    # Get the logits and state from the decoder\n",
    "    logits, state = decoder(context,\n",
    "                            next_token,\n",
    "                            state = state,\n",
    "                            return_state = True)\n",
    "    \n",
    "    # Trim the intermediate dimension \n",
    "    logits = logits[:, -1, :]\n",
    "\n",
    "    if temperature == 0.0: \n",
    "        next_token = tf.argmax(logits, axis = -1)\n",
    "    else:\n",
    "        logits = logits / temperature \n",
    "        next_token = tf.random.categorical(logits, num_samples=1)\n",
    "    \n",
    "    # Trim dimensions of size 1\n",
    "    logits = tf.squeeze(logits)\n",
    "    next_token = tf.squeeze(next_token)\n",
    "    \n",
    "    # Get the logit of the selected next_token\n",
    "    logit = logits[next_token].numpy()\n",
    "    \n",
    "    # Reshape to (1,1) since this is the expected shape for text encoded as TF tensors\n",
    "    next_token = tf.reshape(next_token, shape = (1, 1))\n",
    "    \n",
    "    # If next_token is End-of-Sentence token you are done\n",
    "    if next_token == 3: # [EOS]\n",
    "        done = True\n",
    "    \n",
    "    return next_token, logit, state, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "19f3460b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next token: [[330]]\n",
      "Word:[[b'l\\xc3\\xa2u']]\n",
      "Logit: -15.4154\n",
      "Done? False\n"
     ]
    }
   ],
   "source": [
    "# PROCESS SENTENCE TO TRANSLATE AND ENCODE\n",
    "\n",
    "# A sentence you wish to translate\n",
    "eng_sentence = \"I miss her\"\n",
    "\n",
    "# Convert it to a tensor\n",
    "texts = tf.convert_to_tensor(eng_sentence)[tf.newaxis]\n",
    "\n",
    "# Vectorize it and pass it through the encoder\n",
    "context = eng_vectorizer(texts).to_tensor()\n",
    "context = encoder(context)\n",
    "\n",
    "# SET STATE OF THE DECODER\n",
    "# Next token is Start-of-Sentence since you are starting fresh\n",
    "next_token = tf.fill((1, 1), 2) #[SOS]\n",
    "\n",
    "# Hidden and Cell states of the LSTM can be mocked using uniform samples\n",
    "state = [tf.random.uniform((1, UNITS)), tf.random.uniform((1, UNITS))]\n",
    "done = False \n",
    "\n",
    "next_token, logit, state, done = generate_next_token(decoder, context, next_token, state, done , temperature = 0.5)\n",
    "print(f\"Next token: {next_token}\\nWord:{vie_id2word(next_token)}\\nLogit: {logit:.4f}\\nDone? {done}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4483d4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(model, \n",
    "              text, \n",
    "              max_length = 15,\n",
    "              temperature = 0.0):\n",
    "    \"\"\"Translate a English sentence to Vietnamese\n",
    "\n",
    "    Args:\n",
    "        model (tf.keras.Model): The translator model\n",
    "        text (str): The English context sentence\n",
    "        max_length (int, optional): The maximum length of the translation. Defaults to 15.\n",
    "        temperature (float, optional): Randomness of the translation. Defaults to 0.0.\n",
    "    \n",
    "    Returns:\n",
    "        tuple(str, np.float, tf.Tensor): The translation, logit that predicted [EOS] token, and the tokenizeed translation\n",
    "    \"\"\"\n",
    "    tokens, logits = list(), list()\n",
    "    \n",
    "    # convert the string to tensor\n",
    "    text = tf.convert_to_tensor(text)[tf.newaxis]\n",
    "    \n",
    "    # vectorize the string tensor \n",
    "    context = eng_vectorizer(text).to_tensor()\n",
    "    \n",
    "    # get the encoded context\n",
    "    context = model.encoder(context)\n",
    "    \n",
    "\n",
    "    # [SOS]\n",
    "    next_token = tf.fill((1, 1), 2)\n",
    "    \n",
    "    # initial hidden states(h0) and cell states(c0) will be tensor of 0 with dim (1, UNITS)\n",
    "    state = [tf.zeros((1, UNITS)), tf.zeros((1, UNITS))]\n",
    "    \n",
    "    # done flag \n",
    "    done = False \n",
    "    for i in range(max_length):\n",
    "        try:\n",
    "            next_token, logit, state, done = generate_next_token(\n",
    "                decoder = model.decoder,\n",
    "                context = context, \n",
    "                next_token = next_token,\n",
    "                state = state, \n",
    "                done = done, \n",
    "                temperature = temperature\n",
    "            )\n",
    "        except:\n",
    "            raise Exception(\"Problem generating the next token\")\n",
    "\n",
    "        # check done \n",
    "        if done:\n",
    "            break \n",
    "            \n",
    "        # append the result \n",
    "        tokens.append(next_token)\n",
    "        logits.append(logit)\n",
    "\n",
    "    # concatenate all token into a tensor \n",
    "    tokens = tf.concat(tokens, axis = - 1)\n",
    "    \n",
    "    # convert translated tokens to text\n",
    "    translation = vect_to_text(tokens, vie_id2word)\n",
    "    \n",
    "    return translation, logits[-1], tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dd099671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature: 0.4\n",
      "Original sentence: Please let me go\n",
      "Translation: làm tôi trả tiền tôi đi .\n",
      "Translation tokens:[[ 16   5 239 103   5  22   4]]\n",
      "Logit: -5.307931900024414\n"
     ]
    }
   ],
   "source": [
    "temp = 0.4\n",
    "test_eng_sentence = \"Please let me go\"\n",
    "\n",
    "translation, logit, tokens = translate(translator, test_eng_sentence, temperature = temp)\n",
    "\n",
    "\n",
    "print(f\"Temperature: {temp}\")\n",
    "print(f\"Original sentence: {test_eng_sentence}\")\n",
    "print(f\"Translation: {translation}\")\n",
    "print(f\"Translation tokens:{tokens}\")\n",
    "print(f\"Logit: {logit}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4dcbe821",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples(model, text, n_samples = 4, temperature = 0.6):\n",
    "    \"\"\"\n",
    "    Generate n tranlsation for text\n",
    "\n",
    "    Args:\n",
    "        model (tf.keras.Model): The translate model\n",
    "        text (str): The English context sentence\n",
    "        n_samples (int, optional): Numer of samples need to be generated. Defaults to 4.\n",
    "        temperature (float, optional): Randomess. Defaults to 0.6.\n",
    "\n",
    "    Return: \n",
    "        tuple(list, float): list of setences (tokenized) and its log probs\n",
    "    \"\"\"\n",
    "    samples, log_probs = list(), list()\n",
    "    for _  in range(n_samples):\n",
    "        translation, log_p, sample = translate(model, text, temperature = temperature)\n",
    "        samples.append(np.squeeze(sample.numpy().tolist()))\n",
    "        log_probs.append(log_p)\n",
    "    return samples, log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "56bec7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated tensor: [ 16  29   5  29   5  54 334 284 209   4] has logit: -3.174\n",
      "Translated tensor: [ 85 116   5 273   4] has logit: -1.283\n",
      "Translated tensor: [ 85  29   5  22 129   4] has logit: -2.125\n",
      "Translated tensor: [ 85  29   5 222  22   4] has logit: -0.908\n"
     ]
    }
   ],
   "source": [
    "samples, log_probs = generate_samples(translator, test_eng_sentence)\n",
    "\n",
    "for s, l in zip(samples, log_probs):\n",
    "    print(f\"Translated tensor: {s} has logit: {l:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1f621517",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rouge1_similarity(candidate, reference):\n",
    "    \"\"\"\n",
    "    Compmute ROUGE1 score between 2 token lists\n",
    "\n",
    "    Args:\n",
    "        candidate (list[int]): Tokenized candidate translation\n",
    "        reference (list[int]): Tokenized reference translation\n",
    "    \n",
    "    Return:\n",
    "        float: Overlap between the two token lists\n",
    "    \"\"\"\n",
    "    candiate_word_counts = Counter(candidate)\n",
    "    reference_word_counts = Counter(reference) \n",
    "    overlap = 0\n",
    "    \n",
    "    for token in candidate: \n",
    "        can_token_count = candiate_word_counts[token]\n",
    "        ref_token_count = reference_word_counts[token]\n",
    "        overlap += min(ref_token_count, can_token_count)\n",
    "    \n",
    "    precision = overlap / sum(candiate_word_counts.values())\n",
    "    recall = overlap / sum(reference_word_counts.values())\n",
    "    \n",
    "    f1_score = 0 # if precision + recall = 0 \n",
    "    if precision + recall != 0 : \n",
    "        f1_score = 2 * precision * recall / (precision + recall)\n",
    "    \n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2b2f55c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8571428571428571\n"
     ]
    }
   ],
   "source": [
    "l1 = [1, 2, 3]\n",
    "l2 = [1, 2, 3, 4]\n",
    "print(rouge1_similarity(l1, l2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a5e49da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_overlap(samples):\n",
    "    \"\"\"\n",
    "    Compute the mean of each candiate sentence in the samples\n",
    "\n",
    "    Args:\n",
    "        samples (list[list[int]]): Tokenized version of translated sentences\n",
    "    \n",
    "    Returns:\n",
    "    list[float]: Scores at each index\n",
    "    \"\"\"\n",
    "    n_samples = len(samples)\n",
    "    scores = []\n",
    "    for i1, c1 in enumerate(samples):\n",
    "        s = 0 # save the current mean score\n",
    "        for i2, c2 in enumerate(samples):\n",
    "            if i2 == i1: continue \n",
    "            s += rouge1_similarity(c1, c2)\n",
    "        \n",
    "        # get average \n",
    "        s /= (n_samples - 1)\n",
    "        \n",
    "        # round to 3 decimal points \n",
    "        s = round(s, 3)\n",
    "        \n",
    "        # save to dict \n",
    "        scores.append(s)\n",
    "    \n",
    "    return scores\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7f566832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.619, 0.762, 0.714]\n"
     ]
    }
   ],
   "source": [
    "l1 = [1, 2, 3]\n",
    "l2 = [1, 2, 4]\n",
    "l3 = [1, 2, 4, 5]\n",
    "\n",
    "print(average_overlap([l1, l2, l3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0e6ae63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_avg_overlap(samples,\n",
    "                         log_probs):\n",
    "    \"\"\"\n",
    "    Compute the weighted average of each candidate sentence in samples\n",
    "\n",
    "    Args:\n",
    "        samples (list[list[int]]): Tokenized version of translated sentences\n",
    "        log_probs (list[int]): log prob of each sentence\n",
    "        \n",
    "    Returns:\n",
    "    list[float]: Scores at each index\n",
    "    \"\"\"\n",
    "    n_samples = len(samples)\n",
    "    scores = []\n",
    "    for i1, c1 in enumerate(samples):\n",
    "        s = 0 # save the current mean score\n",
    "        \n",
    "        for i2, (c2, c2_logp) in enumerate(zip(samples, log_probs)):\n",
    "            if i2 == i1: continue \n",
    "            \n",
    "            # convert log prob to linear scale \n",
    "            c2_p = float(np.exp(c2_logp))\n",
    "            \n",
    "            # compute score\n",
    "            s += rouge1_similarity(c1, c2)\n",
    "            \n",
    "            # weight for it \n",
    "            s *= c2_p\n",
    "        \n",
    "        # get average \n",
    "        s /= (n_samples - 1)\n",
    "        \n",
    "        # round to 3 decimal points \n",
    "        s = round(s, 3)\n",
    "        \n",
    "        # save to dict \n",
    "        scores.append(s)\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eba85aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.142, 1.526, 1.044]\n"
     ]
    }
   ],
   "source": [
    "l1 = [1, 2, 3]\n",
    "l2 = [1, 2, 4]\n",
    "l3 = [1, 2, 4, 5]\n",
    "log_probs = [0.4, 0.2, 0.5]\n",
    "\n",
    "print(weighted_avg_overlap([l1, l2, l3], log_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6a62843c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mbr_decode(model,\n",
    "               text,\n",
    "               n_samples = 5, \n",
    "               temperature = 0.6):\n",
    "    # generatre samples\n",
    "    samples, log_probs = generate_samples(model, text, n_samples, temperature)\n",
    "    \n",
    "    # compute scores\n",
    "    scores = weighted_avg_overlap(samples, log_probs)\n",
    "    \n",
    "    # get the highest score translation \n",
    "    id = np.argmax(scores)\n",
    "    translation = samples[id]\n",
    "    \n",
    "    return translation, samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4007cd8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context sentence\n",
      "Please let me go \n",
      "\n",
      "Tranlsation candidates:\n",
      "hãy cho tôi thêm năm .\n",
      "hãy cho tôi suy thêm tuần hôm nay .\n",
      "làm cho tôi bình nửa thế trời tò diện .\n",
      "làm cho tôi chút đi .\n",
      "làm tôi hãy trả việc tôi đi lạc đa .\n",
      "\n",
      "Selected translation\n",
      "làm cho tôi chút đi .\n"
     ]
    }
   ],
   "source": [
    "translation, candidates = mbr_decode(translator, test_eng_sentence, 5)\n",
    "print(\"Context sentence\")\n",
    "print(test_eng_sentence, '\\n')\n",
    "\n",
    "print(\"Tranlsation candidates:\")\n",
    "for c in candidates:\n",
    "    print(vect_to_text(c, vie_id2word))\n",
    "\n",
    "print(\"\\nSelected translation\")\n",
    "print(vect_to_text(translation, vie_id2word))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cu118",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
